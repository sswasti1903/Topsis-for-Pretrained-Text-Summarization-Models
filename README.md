# Topsis-for-Pretrained-Text-Summarization-Models
Based on their evaluation metrics, a repository to show Topsis rankings and scores for pre-trained Text Summarization Models.

## **1. Methodology**
1. Select any five pre-trained models from Hugging Face.
2. Generate responses to a few prompt inputs.
3. Evaluate the pre-trained models based on the following parameters: BLEU, ROUGE-1, ROUGE-2, ROUGE-L score.
4. Compare the pre-trained models by applying Topsis and find ranking.


## **2. Input and Output**
<p>Input</p>
<img src="results_img.png" width="70%" height="60%">

<p>Output</p>
<img src="topsis_img.png" width="100%" height="80%">


## **3. Evaluation Metrics**
1. BLEU
2. ROUGE-1 Score
3. ROUGE-2 Score
4. ROUGE-L Score


## **4. Pre-Trained Models Used**
1. cnicu/t5-small-booksum
2. sshleifer/distilbart-cnn-6-6
3. Falconsai/medical_summarization
4. Cohee/bart-factbook-summarization
5. lidiya/bart-large-xsum-samsum


## **5. Result**
<img src="img1.png" width="100%" height="100%">
